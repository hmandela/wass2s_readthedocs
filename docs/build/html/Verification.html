

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>WAS_Verification Class &mdash; wass2s 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=01f34227"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="wass2s api" href="mme.html" />
    <link rel="prev" title="Models Modules" href="Models.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            wass2s
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="Usage.html">Usage</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="Usage.html#modules">Modules</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="Download.html">Download module</a></li>
<li class="toctree-l3"><a class="reference internal" href="Processing.html">Processing Modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="cv.html">Quantifying uncertainty via cross-validation</a></li>
<li class="toctree-l3"><a class="reference internal" href="Models.html">Models Modules</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">WAS_Verification Class</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deterministic-metrics">Deterministic Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#probabilistic-metrics">Probabilistic Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ensemble-metrics">Ensemble Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tercile-probability-computation">Tercile Probability Computation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gcm-validation">GCM Validation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#annual-year-validation">Annual Year Validation</a></li>
<li class="toctree-l3"><a class="reference internal" href="mme.html">wass2s api</a></li>
<li class="toctree-l3"><a class="reference internal" href="api.html">wass2s api</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api.html">wass2s api</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">wass2s</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="Usage.html">Usage</a></li>
      <li class="breadcrumb-item active">WAS_Verification Class</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/Verification.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p><strong>Verification Module</strong></p>
<p>The Verification module provides tools for evaluating the performance of climate forecasts using a variety of deterministic, probabilistic, and ensemble-based metrics. It is implemented in the <cite>was_verification.py</cite> module and leverages the <cite>WAS_Verification</cite> class to compute metrics such as Kling-Gupta Efficiency (KGE), Pearson Correlation, Ranked Probability Skill Score (RPSS), and Continuous Ranked Probability Score (CRPS). The module also includes visualization utilities for plotting scores, reliability diagrams, and ROC curves.</p>
<p>This module is designed to work with gridded climate data, typically stored in <cite>xarray</cite> DataArrays, and supports parallel computation using <cite>dask</cite> for efficiency with large datasets.</p>
<p><strong>Prerequisites</strong></p>
<ul class="simple">
<li><p><strong>xarray</strong>: For handling multi-dimensional data arrays.</p></li>
<li><p><strong>numpy</strong>: For numerical computations.</p></li>
<li><p><strong>scipy</strong>: For statistical functions.</p></li>
<li><p><strong>scikit-learn</strong>: For metrics and utilities like ROC curves and one-hot encoding.</p></li>
<li><p><strong>xskillscore</strong>: For ensemble-based metrics like CRPS.</p></li>
<li><p><strong>matplotlib</strong> and <strong>cartopy</strong>: For plotting maps and diagrams.</p></li>
<li><p><strong>properscoring</strong>: For scoring probabilistic forecasts.</p></li>
<li><p><strong>dask</strong>: For parallel computing.</p></li>
</ul>
<section id="was-verification-class">
<h1>WAS_Verification Class<a class="headerlink" href="#was-verification-class" title="Link to this heading"></a></h1>
<p>The <cite>WAS_Verification</cite> class is the core of the Verification module, providing methods to compute and visualize various performance metrics for climate forecasts.</p>
<p><strong>Initialization</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">wass2s.was_verification</span> <span class="kn">import</span> <span class="n">WAS_Verification</span>

<span class="c1"># Initialize with a distribution method for probabilistic forecasts</span>
<span class="n">verifier</span> <span class="o">=</span> <span class="n">WAS_Verification</span><span class="p">(</span><span class="n">dist_method</span><span class="o">=</span><span class="s2">&quot;gamma&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Parameters</strong></p>
<ul class="simple">
<li><p><cite>dist_method</cite>: Specifies the distribution method for computing tercile probabilities. Options include:
- <cite>“t”</cite>: Student’s t-based method.
- <cite>“gamma”</cite>: Gamma distribution-based method (default).
- <cite>“normal”</cite>: Normal distribution-based method.
- <cite>“lognormal”</cite>: Lognormal distribution-based method.
- <cite>“weibull_min”</cite>: Weibull minimum distribution-based method.
- <cite>“nonparam”</cite>: Non-parametric method using historical errors.</p></li>
</ul>
<p><strong>Available Metrics</strong></p>
<p>The class defines a dictionary of scoring metrics with metadata, including:</p>
<ul class="simple">
<li><p><strong>Deterministic Metrics</strong>:
- <cite>KGE</cite>: Kling-Gupta Efficiency (-1 to 1).
- <cite>Pearson</cite>: Pearson Correlation Coefficient (-1 to 1).
- <cite>IOA</cite>: Index of Agreement (0 to 1).
- <cite>MAE</cite>: Mean Absolute Error (0 to 100).
- <cite>RMSE</cite>: Root Mean Square Error (0 to 100).
- <cite>NSE</cite>: Nash-Sutcliffe Efficiency (None to 1).
- <cite>TAYLOR_DIAGRAM</cite>: Taylor Diagram (visualization).</p></li>
<li><p><strong>Probabilistic Metrics</strong>:
- <cite>GROC</cite>: Generalized Receiver Operating Characteristic (0 to 1).
- <cite>RPSS</cite>: Ranked Probability Skill Score (-1 to 1).
- <cite>IGS</cite>: Ignorance Score (0 to None).
- <cite>RES</cite>: Resolution Score (0 to None).
- <cite>REL</cite>: Reliability Score (None to None).
- <cite>RELIABILITY_DIAGRAM</cite>: Reliability Diagram (visualization).
- <cite>ROC_CURVE</cite>: Receiver Operating Characteristic Curve (visualization).</p></li>
<li><p><strong>Ensemble Metrics</strong>:
- <cite>CRPS</cite>: Continuous Ranked Probability Score (0 to 100).</p></li>
</ul>
<p><strong>Metadata Access</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">metadata</span> <span class="o">=</span> <span class="n">verifier</span><span class="o">.</span><span class="n">get_scores_metadata</span><span class="p">()</span>
</pre></div>
</div>
<p>This returns a dictionary containing the name, range, type, colormap, and computation function for each metric.</p>
</section>
<section id="deterministic-metrics">
<h1>Deterministic Metrics<a class="headerlink" href="#deterministic-metrics" title="Link to this heading"></a></h1>
<p>Deterministic metrics evaluate the performance of point forecasts against observations. They are computed using the <cite>compute_deterministic_score</cite> method, which applies a scoring function over <cite>xarray</cite> DataArrays.</p>
<p><strong>Example Usage</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute Pearson Correlation</span>
<span class="n">pearson_score</span> <span class="o">=</span> <span class="n">verifier</span><span class="o">.</span><span class="n">compute_deterministic_score</span><span class="p">(</span>
    <span class="n">verifier</span><span class="o">.</span><span class="n">pearson_corr</span><span class="p">,</span> <span class="n">obs_data</span><span class="p">,</span> <span class="n">model_data</span>
<span class="p">)</span>

<span class="c1"># Plot the score</span>
<span class="n">verifier</span><span class="o">.</span><span class="n">plot_model_score</span><span class="p">(</span><span class="n">pearson_score</span><span class="p">,</span> <span class="s2">&quot;Pearson&quot;</span><span class="p">,</span> <span class="n">dir_save_score</span><span class="o">=</span><span class="s2">&quot;./scores&quot;</span><span class="p">,</span> <span class="n">figure_name</span><span class="o">=</span><span class="s2">&quot;Pearson_Score&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Key Methods</strong></p>
<ul class="simple">
<li><p><cite>kling_gupta_efficiency</cite>: Computes KGE, balancing correlation, bias, and variability.</p></li>
<li><p><cite>pearson_corr</cite>: Computes Pearson Correlation Coefficient.</p></li>
<li><p><cite>index_of_agreement</cite>: Computes IOA, measuring agreement between predictions and observations.</p></li>
<li><p><cite>mean_absolute_error</cite>: Computes MAE, the average absolute difference.</p></li>
<li><p><cite>root_mean_square_error</cite>: Computes RMSE, the square root of mean squared differences.</p></li>
<li><p><cite>nash_sutcliffe_efficiency</cite>: Computes NSE, comparing prediction errors to the mean of observations.</p></li>
<li><p><cite>taylor_diagram</cite>: Placeholder for Taylor Diagram visualization (to be implemented).</p></li>
</ul>
<p><strong>Plotting</strong></p>
<p>The <cite>plot_model_score</cite> method visualizes deterministic scores on a map using <cite>cartopy</cite>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">verifier</span><span class="o">.</span><span class="n">plot_model_score</span><span class="p">(</span><span class="n">score_result</span><span class="p">,</span> <span class="s2">&quot;KGE&quot;</span><span class="p">,</span> <span class="n">dir_save_score</span><span class="o">=</span><span class="s2">&quot;./scores&quot;</span><span class="p">,</span> <span class="n">figure_name</span><span class="o">=</span><span class="s2">&quot;KGE_Model&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The <cite>plot_models_score</cite> method plots multiple model scores in a grid.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model_metrics</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;model1&quot;</span><span class="p">:</span> <span class="n">score_result1</span><span class="p">,</span>
    <span class="s2">&quot;model2&quot;</span><span class="p">:</span> <span class="n">score_result2</span>
<span class="p">}</span>
<span class="n">verifier</span><span class="o">.</span><span class="n">plot_models_score</span><span class="p">(</span><span class="n">model_metrics</span><span class="p">,</span> <span class="s2">&quot;Pearson&quot;</span><span class="p">,</span> <span class="n">dir_save_score</span><span class="o">=</span><span class="s2">&quot;./scores&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="probabilistic-metrics">
<h1>Probabilistic Metrics<a class="headerlink" href="#probabilistic-metrics" title="Link to this heading"></a></h1>
<p>Probabilistic metrics evaluate the performance of forecasts that provide probabilities for tercile categories (below-normal, near-normal, above-normal). These are computed using the <cite>compute_probabilistic_score</cite> method.</p>
<p><strong>Example Usage</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute tercile probabilities</span>
<span class="n">proba_forecast</span> <span class="o">=</span> <span class="n">verifier</span><span class="o">.</span><span class="n">gcm_compute_prob</span><span class="p">(</span><span class="n">obs_data</span><span class="p">,</span> <span class="n">clim_year_start</span><span class="o">=</span><span class="mi">1981</span><span class="p">,</span> <span class="n">clim_year_end</span><span class="o">=</span><span class="mi">2010</span><span class="p">,</span> <span class="n">hindcast_det</span><span class="o">=</span><span class="n">model_data</span><span class="p">)</span>

<span class="c1"># Compute RPSS</span>
<span class="n">rpss_score</span> <span class="o">=</span> <span class="n">verifier</span><span class="o">.</span><span class="n">compute_probabilistic_score</span><span class="p">(</span>
    <span class="n">verifier</span><span class="o">.</span><span class="n">calculate_rpss</span><span class="p">,</span> <span class="n">obs_data</span><span class="p">,</span> <span class="n">proba_forecast</span><span class="p">,</span> <span class="n">clim_year_start</span><span class="o">=</span><span class="mi">1981</span><span class="p">,</span> <span class="n">clim_year_end</span><span class="o">=</span><span class="mi">2010</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>Key Methods</strong></p>
<ul class="simple">
<li><p><cite>classify</cite>: Classifies data into terciles based on climatology.</p></li>
<li><p><cite>compute_class</cite>: Computes tercile class labels for observations.</p></li>
<li><p><cite>calculate_groc</cite>: Computes GROC, averaging AUC across tercile categories.</p></li>
<li><p><cite>calculate_rpss</cite>: Computes RPSS, comparing forecast probabilities to climatology.</p></li>
<li><p><cite>ignorance_score</cite>: Computes Ignorance Score per Weijs (2010).</p></li>
<li><p><cite>resolution_score_grid</cite>: Computes Resolution Score, measuring how forecasts differ from climatology.</p></li>
<li><p><cite>reliability_score_grid</cite>: Computes Reliability Score, assessing forecast calibration.</p></li>
<li><p><cite>reliability_diagram</cite>: Plots Reliability Diagrams for each tercile category.</p></li>
<li><p><cite>plot_roc_curves</cite>: Plots ROC Curves with confidence intervals for each tercile.</p></li>
</ul>
<p><strong>Visualization</strong></p>
<p>Reliability Diagrams and ROC Curves are generated for probabilistic forecasts.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot Reliability Diagram</span>
<span class="n">verifier</span><span class="o">.</span><span class="n">reliability_diagram</span><span class="p">(</span>
    <span class="n">modelname</span><span class="o">=</span><span class="s2">&quot;Model1&quot;</span><span class="p">,</span> <span class="n">dir_to_save_score</span><span class="o">=</span><span class="s2">&quot;./scores&quot;</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">obs_data</span><span class="p">,</span> <span class="n">y_probs</span><span class="o">=</span><span class="n">proba_forecast</span><span class="p">,</span>
    <span class="n">clim_year_start</span><span class="o">=</span><span class="mi">1981</span><span class="p">,</span> <span class="n">clim_year_end</span><span class="o">=</span><span class="mi">2010</span>
<span class="p">)</span>

<span class="c1"># Plot ROC Curves with 95% confidence intervals</span>
<span class="n">verifier</span><span class="o">.</span><span class="n">plot_roc_curves</span><span class="p">(</span>
    <span class="n">modelname</span><span class="o">=</span><span class="s2">&quot;Model1&quot;</span><span class="p">,</span> <span class="n">dir_to_save_score</span><span class="o">=</span><span class="s2">&quot;./scores&quot;</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">obs_data</span><span class="p">,</span> <span class="n">y_probs</span><span class="o">=</span><span class="n">proba_forecast</span><span class="p">,</span>
    <span class="n">clim_year_start</span><span class="o">=</span><span class="mi">1981</span><span class="p">,</span> <span class="n">clim_year_end</span><span class="o">=</span><span class="mi">2010</span><span class="p">,</span> <span class="n">n_bootstraps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">ci</span><span class="o">=</span><span class="mf">0.95</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="ensemble-metrics">
<h1>Ensemble Metrics<a class="headerlink" href="#ensemble-metrics" title="Link to this heading"></a></h1>
<p>Ensemble metrics evaluate forecasts with multiple members, such as those from GCMs. The primary metric is CRPS, computed using <cite>xskillscore</cite>.</p>
<p><strong>Example Usage</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute CRPS for ensemble forecast</span>
<span class="n">crps_score</span> <span class="o">=</span> <span class="n">verifier</span><span class="o">.</span><span class="n">compute_crps</span><span class="p">(</span><span class="n">obs_data</span><span class="p">,</span> <span class="n">model_data</span><span class="p">,</span> <span class="n">member_dim</span><span class="o">=</span><span class="s1">&#39;number&#39;</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="s2">&quot;T&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Key Methods</strong></p>
<ul class="simple">
<li><p><cite>compute_crps</cite>: Computes CRPS for ensemble forecasts, measuring the difference between predicted and observed distributions.</p></li>
</ul>
</section>
<section id="tercile-probability-computation">
<h1>Tercile Probability Computation<a class="headerlink" href="#tercile-probability-computation" title="Link to this heading"></a></h1>
<p>The module provides multiple methods to compute tercile probabilities for probabilistic forecasts, based on different distributional assumptions.</p>
<p><strong>Key Methods</strong></p>
<ul class="simple">
<li><p><cite>calculate_tercile_probabilities</cite>: Uses Student’s t-distribution.</p></li>
<li><p><cite>calculate_tercile_probabilities_gamma</cite>: Uses Gamma distribution.</p></li>
<li><p><cite>calculate_tercile_probabilities_normal</cite>: Uses Normal distribution.</p></li>
<li><p><cite>calculate_tercile_probabilities_lognormal</cite>: Uses Lognormal distribution.</p></li>
<li><p><cite>calculate_tercile_probabilities_weibull_min</cite>: Uses Weibull minimum distribution.</p></li>
<li><p><cite>calculate_tercile_probabilities_nonparametric</cite>: Uses historical errors for a non-parametric approach.</p></li>
</ul>
<p><strong>Example Usage</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute probabilities using Gamma distribution</span>
<span class="n">hindcast_prob</span> <span class="o">=</span> <span class="n">verifier</span><span class="o">.</span><span class="n">gcm_compute_prob</span><span class="p">(</span>
    <span class="n">Predictant</span><span class="o">=</span><span class="n">obs_data</span><span class="p">,</span> <span class="n">clim_year_start</span><span class="o">=</span><span class="mi">1981</span><span class="p">,</span> <span class="n">clim_year_end</span><span class="o">=</span><span class="mi">2010</span><span class="p">,</span> <span class="n">hindcast_det</span><span class="o">=</span><span class="n">model_data</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The <cite>gcm_compute_prob</cite> method selects the appropriate distribution based on the <cite>dist_method</cite> parameter.</p>
</section>
<section id="gcm-validation">
<h1>GCM Validation<a class="headerlink" href="#gcm-validation" title="Link to this heading"></a></h1>
<p>The module includes methods to validate General Circulation Model (GCM) forecasts against observations, supporting both deterministic and probabilistic metrics.</p>
<p><strong>Key Methods</strong></p>
<ul class="simple">
<li><p><cite>gcm_validation_compute</cite>: Validates GCM forecasts for multiple models, computing specified metrics.</p></li>
<li><p><cite>weighted_gcm_forecasts</cite>: Combines forecasts from multiple models using weights based on a performance metric (e.g., GROC).</p></li>
</ul>
<p><strong>Example Usage</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Validate GCM forecasts</span>
<span class="n">models_files_path</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;model1&quot;</span><span class="p">:</span> <span class="s2">&quot;path/to/model1.nc&quot;</span><span class="p">,</span>
    <span class="s2">&quot;model2&quot;</span><span class="p">:</span> <span class="s2">&quot;path/to/model2.nc&quot;</span>
<span class="p">}</span>
<span class="n">x_metric</span> <span class="o">=</span> <span class="n">verifier</span><span class="o">.</span><span class="n">gcm_validation_compute</span><span class="p">(</span>
    <span class="n">models_files_path</span><span class="o">=</span><span class="n">models_files_path</span><span class="p">,</span> <span class="n">Obs</span><span class="o">=</span><span class="n">obs_data</span><span class="p">,</span> <span class="n">score</span><span class="o">=</span><span class="s2">&quot;Pearson&quot;</span><span class="p">,</span>
    <span class="n">month_of_initialization</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">clim_year_start</span><span class="o">=</span><span class="mi">1981</span><span class="p">,</span> <span class="n">clim_year_end</span><span class="o">=</span><span class="mi">2010</span><span class="p">,</span>
    <span class="n">dir_to_save_roc_reliability</span><span class="o">=</span><span class="s2">&quot;./scores&quot;</span><span class="p">,</span> <span class="n">lead_time</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Compute weighted GCM forecasts</span>
<span class="n">hindcast_det</span><span class="p">,</span> <span class="n">hindcast_prob</span><span class="p">,</span> <span class="n">forecast_prob</span> <span class="o">=</span> <span class="n">verifier</span><span class="o">.</span><span class="n">weighted_gcm_forecasts</span><span class="p">(</span>
    <span class="n">Obs</span><span class="o">=</span><span class="n">obs_data</span><span class="p">,</span> <span class="n">best_models</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;model1_MarIc_JFM_1&quot;</span><span class="p">:</span> <span class="n">score1</span><span class="p">},</span> <span class="n">scores</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;GROC&quot;</span><span class="p">:</span> <span class="n">x_metric</span><span class="p">},</span>
    <span class="n">lead_time</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">model_dir</span><span class="o">=</span><span class="s2">&quot;./models&quot;</span><span class="p">,</span> <span class="n">clim_year_start</span><span class="o">=</span><span class="mi">1981</span><span class="p">,</span> <span class="n">clim_year_end</span><span class="o">=</span><span class="mi">2010</span><span class="p">,</span> <span class="n">variable</span><span class="o">=</span><span class="s2">&quot;PRCP&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="annual-year-validation">
<h1>Annual Year Validation<a class="headerlink" href="#annual-year-validation" title="Link to this heading"></a></h1>
<p>The module provides utilities to validate forecasts for a specific year, including ratio-to-average classification and RPSS computation.</p>
<p><strong>Key Methods</strong></p>
<ul class="simple">
<li><p><cite>ratio_to_average</cite>: Classifies forecast data relative to the climatological mean into categories (e.g., Well Above Average, Near Average).</p></li>
<li><p><cite>compute_one_year_rpss</cite>: Computes RPSS for a specific year and visualizes it on a map.</p></li>
</ul>
<p><strong>Example Usage</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Classify ratio to average for a specific year</span>
<span class="n">verifier</span><span class="o">.</span><span class="n">ratio_to_average</span><span class="p">(</span><span class="n">predictant</span><span class="o">=</span><span class="n">obs_data</span><span class="p">,</span> <span class="n">clim_year_start</span><span class="o">=</span><span class="mi">1981</span><span class="p">,</span> <span class="n">clim_year_end</span><span class="o">=</span><span class="mi">2010</span><span class="p">,</span> <span class="n">year</span><span class="o">=</span><span class="mi">2020</span><span class="p">)</span>

<span class="c1"># Compute RPSS for a specific year</span>
<span class="n">verifier</span><span class="o">.</span><span class="n">compute_one_year_rpss</span><span class="p">(</span>
    <span class="n">obs</span><span class="o">=</span><span class="n">obs_data</span><span class="p">,</span> <span class="n">prob_pred</span><span class="o">=</span><span class="n">proba_forecast</span><span class="p">,</span> <span class="n">clim_year_start</span><span class="o">=</span><span class="mi">1981</span><span class="p">,</span> <span class="n">clim_year_end</span><span class="o">=</span><span class="mi">2010</span><span class="p">,</span> <span class="n">year</span><span class="o">=</span><span class="mi">2020</span>
<span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Placeholder Functions</strong>: Some methods (e.g., <cite>taylor_diagram</cite>) are placeholders and require implementation based on specific needs.</p></li>
<li><p><strong>Gridded Data</strong>: The module currently supports only gridded data validation. Non-gridded validation is not implemented.</p></li>
<li><p><strong>Performance</strong>: The use of <cite>dask</cite> ensures efficient computation for large datasets, but users should ensure proper chunking of <cite>xarray</cite> DataArrays.</p></li>
<li><p><strong>Visualization</strong>: Plots are saved to the specified directory and displayed using <cite>matplotlib</cite>. Ensure the output directory exists.</p></li>
</ul>
<p>This documentation provides an overview of the Verification module’s capabilities, along with example usage for key methods. For detailed information on each method, refer to the source code and docstrings in <cite>was_verification.py</cite>.</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Models.html" class="btn btn-neutral float-left" title="Models Modules" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="mme.html" class="btn btn-neutral float-right" title="wass2s api" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Mandela C. M. HOUNGNIBO.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>